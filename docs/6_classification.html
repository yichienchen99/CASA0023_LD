<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CASA0023 Learning Diary - 6&nbsp; Classification 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./7_classification2.html" rel="next">
<link href="./5_GEE.html" rel="prev">
<link href="./image/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./images/casa_logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">CASA0023 Learning Diary</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/yichienchen99/CASA0023_LD"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Classification 1</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Content</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_portfolio.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sensor-WorldView3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_corrections.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Corrections</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_policy.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_GEE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Google Earth Engine</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_classification.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Classification 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_classification2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_temperature.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Temperature</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dictionary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Dictionary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary"><span class="toc-section-number">6.1</span>  Summary</a>
  <ul class="collapse">
  <li><a href="#types-of-image-classification" id="toc-types-of-image-classification" class="nav-link" data-scroll-target="#types-of-image-classification"><span class="toc-section-number">6.1.1</span>  3 types of image classification</a></li>
  <li><a href="#overfitting-of-trees" id="toc-overfitting-of-trees" class="nav-link" data-scroll-target="#overfitting-of-trees"><span class="toc-section-number">6.1.2</span>  Overfitting (of trees)</a></li>
  <li><a href="#questions" id="toc-questions" class="nav-link" data-scroll-target="#questions"><span class="toc-section-number">6.1.3</span>  Questions:</a></li>
  </ul></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="toc-section-number">6.2</span>  Application</a>
  <ul class="collapse">
  <li><a href="#cart-random-forest" id="toc-cart-random-forest" class="nav-link" data-scroll-target="#cart-random-forest"><span class="toc-section-number">6.2.1</span>  CART &amp; Random Forest</a></li>
  <li><a href="#svm" id="toc-svm" class="nav-link" data-scroll-target="#svm"><span class="toc-section-number">6.2.2</span>  SVM</a></li>
  </ul></li>
  <li><a href="#reflection" id="toc-reflection" class="nav-link" data-scroll-target="#reflection"><span class="toc-section-number">6.3</span>  Reflection</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/yichienchen99/CASA0023_LD/edit/main/6_classification.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/yichienchen99/CASA0023_LD/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Classification 1</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="summary" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.1</span> Summary</h2>
<p>Human is good at finding patterns in imagery while computers are not. The patterns / land cover types are useful for analysis, but it is tedious to digitize hugeamount of data manually. Hence, the task is given to the computers, extracting patterns from remote sensing data. Decision trees replicate human decisions on detecting patterns and making conclusions based on given information (expert system!). Multiple methods based on decision trees are developed to classify remote sensing image!</p>
<blockquote class="blockquote">
<p>Image classification: Segregate pixels of a remote sensing image into groups of similar spectral character / spectral categorical classification.</p>
</blockquote>
<section id="types-of-image-classification" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="types-of-image-classification"><span class="header-section-number">6.1.1</span> 3 types of image classification</h3>
<ol type="1">
<li><strong>Unsupervised image classification</strong></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://gisgeography.com/wp-content/uploads/2014/06/unsupervised-diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">(GISGeography, 2014)</figcaption><p></p>
</figure>
</div>
<p>Process: Clustering algorithm: K-means; ISODATA &gt; number of clusters [Fewer clusters have more resembling pixels within groups. More clusters increase the variability within groups. <span class="citation" data-cites="gisgeography2014">(<a href="references.html#ref-gisgeography2014" role="doc-biblioref">GISGeography 2014</a>)</span>] &gt; manually assign land cover classes to each cluster</p>
<p>Not know the class (except the number of cluster) before the process.</p>
<p>Scale: pixel-based</p>
<ol start="2" type="1">
<li><strong>Supervised image classification</strong></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://gisgeography.com/wp-content/uploads/2014/06/supervised-diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">(GISGeography, 2014)</figcaption><p></p>
</figure>
</div>
<p>Process: Select representative training samples for each land cover class &gt; generate a signature file, storing all training samples’ spectral information &gt; use the signature file to run a classification (through one of the classification algorithm: Maximum likelihood (normal distribution/parametric); Density slicing; Nearest neighbor; Minimum-distance; Principal components; Support vector machine (SVM); CART; RF; Iso cluster; Neural network) &gt; pixel assignment &gt; accuracy assessment</p>
<p>Pattern recognition / ML</p>
<p>Scale: pixel-based</p>
<ul>
<li><p>CART: a tree</p></li>
<li><p>RF: merging many trees, trained with bagging methods (to create a combination of learning models which improves the overall result)</p></li>
<li><p>SVM: optimal multi-dimensional decision hyperplane boundary that divides the dataset (test all possibilities of C and Gamma &gt; compare with testing data &gt; choose the combination with best accuracy)</p></li>
</ul>
<ol start="3" type="1">
<li><strong>Object-based image analysis (OBIA)</strong></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://gisgeography.com/wp-content/uploads/2014/06/object-based-diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">(GISGeography, 2014)</figcaption><p></p>
</figure>
</div>
<p>Segmentation algorithms: Multi-resolution segmentation in <a href="http://www.ecognition.com/"><strong>eCognition</strong></a>; The segment mean shift tool in <a href="https://www.arcgis.com/"><strong>ArcGIS</strong></a> &gt; different methods to classify objects (shape; texture; spectral; geographic context; nearest neighbor)</p>
<p>Scale: object-based.. (groups pixels into representative vector shapes/objects)</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>✔️: Esp. good for <strong>high spatial resolution image</strong> by avoiding the noisiness in the outputs of pixel-based methods, more like human in processing patterns. Because OBIA used both spectral and contextual information, it had higher accuracy compared to pixel-based methods (when comparing high and medium spatial resolution imagery) <span class="citation" data-cites="weih">(<a href="references.html#ref-weih" role="doc-biblioref">Weih and Riggan, n.d.</a>)</span>. &gt; gaining popularity due to increasingly amount of high-resolution data available! More in next chapter..</p>
</div>
</div>
</section>
<section id="overfitting-of-trees" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="overfitting-of-trees"><span class="header-section-number">6.1.2</span> Overfitting (of trees)</h3>
<blockquote class="blockquote">
<p>have leaves with one pixel value.. Large difference between predicted values and true value (= high bias = oversimplified model); High variability of a model for a given point (= high variance = not generalize enough)</p>
</blockquote>
<section id="methods-to-balance-bias-and-variance" class="level4" data-number="6.1.2.1">
<h4 data-number="6.1.2.1" class="anchored" data-anchor-id="methods-to-balance-bias-and-variance"><span class="header-section-number">6.1.2.1</span> 2 methods to balance bias and variance</h4>
<ol type="1">
<li><u>Limit the minimum number of pixels in leaves</u> (usually 20 pixels) = top-down, easier to perform, but less mathematically sound.</li>
<li><u>Weakest link pruning with tree score</u> (find the weakest link and delete it) = bottom-up.
<ul>
<li><p>Tree score = SSR + tree penalty (alpha) * T(number of leaves)</p></li>
<li><p>For all data &gt; Use alpha=0 for all data (=full tree = SSR) &gt; increase alpha values &gt; find the Alpha with lowest tree score compared to the full tree when decreasing number of leaves</p></li>
<li><p>Train and test split &gt; Use the Alpha for train data &gt; new trees &gt; put test data in new trees &gt; calculate SSR for test data &gt; find the tree/alpha with the smallest SSR</p></li>
<li><p>Repeat 10 times the above train and test split (using different data combinations) &gt; find the alpha with lowest SSR across 10 repetitions</p></li>
<li><p>Apply this final alpha to the whole data</p></li>
</ul></li>
</ol>
</section>
</section>
<section id="questions" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="questions"><span class="header-section-number">6.1.3</span> Questions:</h3>
<ol type="1">
<li>(Practical) Whats the difference between <em><code>.reduce(ee.Reducer.median())</code></em> and <em><code>.median()</code></em> ??</li>
</ol>
<ul>
<li><p>Tested - mostly no difference in the outputs. but the band name becomes B1_median when using reducer.median..</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image/reducer.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">output using .reduce</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image/median.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">output using .median</figcaption><p></p>
</figure>
</div></li>
</ul>
<ol start="2" type="1">
<li><p>(Practical) Why the background vector map is not consistent with the remote sensing map (like the roads and buildings are not in the same position, see below snapshot)? Will this influence the data selection and accuracy (i assume other datasets may also have different consistency)?</p>
<p><img src="image/inconsistency.png" class="img-fluid"></p></li>
</ol>
<ul>
<li>(A: projection, but dont bother..)</li>
</ul>
<ol start="3" type="1">
<li>(Practical) How to select high and low urban samples for training for supervised classification? (High and low as of albedo or as of density? Based on Andy’s practical workbook, it is albedo. Is this for analyzing the impact of albedo/building material on the micro-climate and earth surface temperature?)</li>
</ol>
<ul>
<li>A: Albedo - building reflectence - analysing the energy and temperature. For density - Google building data - areas of building / area of whole pixel ..</li>
</ul>
<ol start="4" type="1">
<li>(Practical) Error occur when trying to print the training data after selecting different land cover as feature collection: <code>FeatureCollection: Collection query aborted after accumulating over 5000 elements.</code></li>
</ol>
<ul>
<li>Answers <a href="https://gis.stackexchange.com/questions/327644/error-collection-query-aborted-after-accumulating-over-5000-elements">online</a>: when printing the training data, use <code>.limit(5000)</code> to allow printing subset of large data. (not sure if it is the right way to do, but this is the checking process afterall) (and not sure why the data is so large..)</li>
</ul>
</section>
</section>
<section id="application" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="application"><span class="header-section-number">6.2</span> Application</h2>
<p>Classification of EO imagery allows extracting (and predicting) patterns/land cover from the data. This supports analysis on LULC, urban expansion, urban green space, illegal logging, forest fire, etc.</p>
<section id="cart-random-forest" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="cart-random-forest"><span class="header-section-number">6.2.1</span> CART &amp; Random Forest</h3>
<p>As done in the practical, the Sentinel data is classified using CART and RF. When selecting training data (selecting land cover geometries as variables), the false inference on the landuse and the inclusion of other land cover type in the geometry may add noise to the prediction, hence lowering the accuracy. Also, mentioned in the practical, when splitting the data into training and testing (for accuracy testing), the pixels of the selected geometries should be used, rather than the geometry that may add more noise to the results.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image/output1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Output from practical - land cover classification using CART</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image/output2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Output from practical - land cover classification using RF</figcaption><p></p>
</figure>
</div>
<p>The accuracy is not as high as Andy’s output. For RF, the OOB error is 6%.. and the accuracy is 93%. Resubstitution accuracy is 99% (original training data vs.&nbsp;model output). When comparing testing data and model output, the accuracy gives 93%. &gt; Is there any correlations between OOB accuracy and the confusion matrix? (and is it possible to add the legend to the output?)</p>
<p>Comparing two outputs visually, RF captures the difference in high and low urban land cover types better than CART. However, both methods produce outputs that seems like salt and pepper since they are pixel-based. Although the low urban type might exist in the forest (like meterological stations), it might not be necessary when drawing out the forest boundary. Object-based methods could be used for this purpose and for higher resolution building boundary detection for instance.</p>
</section>
<section id="svm" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="svm"><span class="header-section-number">6.2.2</span> SVM</h3>
<p>[Technical / methods] Study by <span class="citation" data-cites="ustuner2015">(<a href="references.html#ref-ustuner2015" role="doc-biblioref">Ustuner, Sanli, and Dixon 2015</a>)</span> constructs the a classification model for agricultural landuse in Turkey using SVM method with <a href="https://www.eoportal.org/satellite-missions/rapideye#rapideye-earth-observation-constellation">RapidEye</a> imargery. With fieldwork and Google Earth information on land cover types, it compares the classification accuracy with the result from Maximum Likelihood Classification (MLC) and internally among the results using different parameters (error penalty (C), gamma, bias term (r), polynomial degree (d)) and kernel types (linear, polynomial, radial basis function, sigmoid). Accuracy is assessed through <a href="https://vitalflux.com/cohen-kappa-score-python-example-machine-learning/?utm_content=cmp-true">kappa coefficient</a> (comparing classified images against ground truth data). Results show that the SVM method outperforms MLC and classification accuracy is influenced by the choice of parameters (and kernels) <span class="citation" data-cites="ustuner2015">(<a href="references.html#ref-ustuner2015" role="doc-biblioref">Ustuner, Sanli, and Dixon 2015</a>)</span>. However, the generalization performance of kernels is influenced by the types of dataset (multi- or hyper-spectural or SAR…) so the conclusion on the best-performing kernel type is not fixed (may vary due to dataset).</p>
<p>[Context / application] The country’s agricultural productivity highly influence its economy. However, the data on agricultural statistics is manually collected by government employees from farmers’ declaration. The data could be unreliable <span class="citation" data-cites="ustuner2015">(<a href="references.html#ref-ustuner2015" role="doc-biblioref">Ustuner, Sanli, and Dixon 2015</a>)</span> and labour-intensive, being not cost-effective for the government. Hence, methods like applying image classification on remote sensing data could provide viable, up-to-data and reliable information for sustainable crop management and planning. Before putting into practice, the sensitivity / reliability of the methods is tested through comparing between different models and within SVM using different parameters and kernels. The best combination of parameter and kernel will be used to inform decisions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image/classified_rapideye.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">(Ustuner, Sanli and Dixon, 2015)</figcaption><p></p>
</figure>
</div>
<p>[Result / recommendation] The above output shows highest accuracy selections (compared to results from other models with different error penalty in the same kernel) of four different kernels from SVM + the result from MLC. Kappa for MS9 = 0.8209, MR5 = 0.8222, ML11 = 0.8223, MP9 = 0.8411. &gt;&gt; Recommendation from the authors: optimum parameters for SVM should be analyzed in detail before choosing one for best classification result.</p>
<p>Another study by <span class="citation" data-cites="pal2005">(<a href="references.html#ref-pal2005" role="doc-biblioref">Pal and Mather 2005</a>)</span> suggests that SVM (using pair-wise comparison for multi-class) achieves higher accuracy than ML and ANN. Nevertheless, effective use depends on the values of a few user‐defined parameters. SVM in dealing with multi-class: n hyperplanes should be determined (comparing one class with the rest &gt; may produce unclassified data, hence lowing the accuracy) or n(n-1)/2 classifiers (pair-wise comparison) <span class="citation" data-cites="pal2005">(<a href="references.html#ref-pal2005" role="doc-biblioref">Pal and Mather 2005</a>)</span>.</p>
</section>
</section>
<section id="reflection" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="reflection"><span class="header-section-number">6.3</span> Reflection</h2>
<ul>
<li><p>Which classifier to choose when performing image classification?</p>
<ul>
<li><p>Reference to literature on relevant themes might hint the direction.</p></li>
<li><p>Performing/comparing different classifiers might just be within several lines of code in GEE. (but.. Does the accuracy means everything? Choosing the one with highest accuracy might not be reproducible in the sense that different regions/time of year/giving new data could yield varying results?)</p></li>
<li><p>The choice of classifier depends on the <strong>intended outcomes</strong> and the <strong>data quality</strong> <span class="citation" data-cites="pragati212020 lanenok2015">(<a href="references.html#ref-pragati212020" role="doc-biblioref">Pragati21 2020</a>; <a href="references.html#ref-lanenok2015" role="doc-biblioref">lanenok 2015</a>)</span>.</p>
<ol type="1">
<li><p>&gt; SVM &gt; sparse data &amp; binary classification &amp; non-linear data (faster and better results, gives <u>distance</u> to boundary)</p></li>
<li><p>&gt; RF &gt; numerical and categorical features &amp; multi-class (gives probability of belonging to class)</p></li>
<li><p>&gt; MLC (parametric) &gt; assuming normally distributed data (not common in LULC data &gt; may results in lower accuracy)</p></li>
</ol></li>
</ul></li>
<li><p>Image classification workflow: DN/reflectance of imagery &gt; (some fieldwork / Google Earth selection of training land cover type data &gt;) being divided based on similarities/closeness to each other using different forms of classifiers/algorithms &gt; revealing/predicting the LULC of the Earth surface &gt; testing accuracy &gt; being used for further analysis / practical applications in assisting decision making</p></li>
<li><p>It seems like performing classification requires some knowledge on the study area’s land cover, especially classifying detailed land cover like agricultural landuse. From the ground truth map, different crop types usually looks similar to human eyes while obtaining different reflectance. Hence, field works would be required to collect in-situ point data using GPS and Google Earth ancillary data could be the additional data source with experts opinions <span class="citation" data-cites="ustuner2015">(<a href="references.html#ref-ustuner2015" role="doc-biblioref">Ustuner, Sanli, and Dixon 2015</a>)</span>.</p></li>
<li><p>Advancement in sensors increases the possibility of recording more data in the imagery, leading to better accuracy in predictions (RedEdge band in RapidEye for example).</p></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-gisgeography2014" class="csl-entry" role="doc-biblioentry">
GISGeography. 2014. <span>“Image Classification Techniques in Remote Sensing.”</span> <a href="https://gisgeography.com/image-classification-techniques-remote-sensing/">https://gisgeography.com/image-classification-techniques-remote-sensing/</a>.
</div>
<div id="ref-lanenok2015" class="csl-entry" role="doc-biblioentry">
lanenok. 2015. <span>“Answer to <span>"</span>When to Use Random Forest over SVM and Vice Versa?<span>"</span>.”</span> <a href="https://datascience.stackexchange.com/a/6855">https://datascience.stackexchange.com/a/6855</a>.
</div>
<div id="ref-pal2005" class="csl-entry" role="doc-biblioentry">
Pal, M., and P. M. Mather. 2005. <span>“Support Vector Machines for Classification in Remote Sensing.”</span> <em>International Journal of Remote Sensing</em> 26 (5): 1007–11. <a href="https://doi.org/10.1080/01431160512331314083">https://doi.org/10.1080/01431160512331314083</a>.
</div>
<div id="ref-pragati212020" class="csl-entry" role="doc-biblioentry">
Pragati21. 2020. <span>“SVM AND RANDOM FOREST: A Case Study.”</span> <a href="https://medium.com/@pandeypragati2112/svm-and-random-forest-a-case-study-6213da5be02f">https://medium.com/@pandeypragati2112/svm-and-random-forest-a-case-study-6213da5be02f</a>.
</div>
<div id="ref-ustuner2015" class="csl-entry" role="doc-biblioentry">
Ustuner, Mustafa, Fusun Balik Sanli, and Barnali Dixon. 2015. <span>“Application of Support Vector Machines for Landuse Classification Using High-Resolution RapidEye Images: A Sensitivity Analysis.”</span> <em>European Journal of Remote Sensing</em> 48 (1): 403–22. <a href="https://doi.org/10.5721/EuJRS20154823">https://doi.org/10.5721/EuJRS20154823</a>.
</div>
<div id="ref-weih" class="csl-entry" role="doc-biblioentry">
Weih, Robert C, and Norman D Riggan. n.d. <span>“OBJECT-BASED CLASSIFICATION VS. PIXEL-BASED CLASSIFICATION: COMPARITIVE IMPORTANCE OF MULTI-RESOLUTION IMAGERY.”</span>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./5_GEE.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Google Earth Engine</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./7_classification2.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Classification 2</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>