[
  {
    "objectID": "1_intro.html#summary",
    "href": "1_intro.html#summary",
    "title": "1  Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 Sensors\n\nPassive (Sun energy reflection in EM wave, don’t emit EM waves)\n\nExample: human eyes, satellite (e.g. Wk2 sensor summary - WorldView-3)\n❌ Influenced by atmosphere Haze (require atmosphere correction, no haze in outer space due to no atmosphere) and scattering (wavelength (e.g. blue sky and orange sunset)), clouds and weather\n✔️does not disturb the object or area of interest\n\nActive (emit and receive EM wave / energy)\n\nExample: radar, x-ray, LiDAR\n✔️ can pass clouds\n\n\n\n\n1.1.2 Four resolutions\n\nSpatial\n\nthe size of the raster grid per pixel (e.g. 20cm or 30m)\nLow spatial resolution: fast (short revisit time); High spatial resolution: costly\n\nSpectral\n\nSpectral signature (EM): Objects on Earth have difference wavelengths = Bands\n\nMulti-spectral data / image\nHyperspectral data / image: stack all colour bands\n\nTrue colour (human eye can see) and false colour (human cant see)\n❌ atmospheric window (atmospheric transmission / opacity, e.g. water vapour, ozone, CO…)\nSpectroradiometer\n\nRadiometric\n\nidentify differences in light or reflectance, in practice this is the range of possible values.\nIncrease bit -> increase possible values\n8-bit sensor: 0-255; 11-bit sensor: 0-2047\nSentinel-2 is 12-bit (brightness levels from 0 – 4095) (beyond True Colour Image (TCI) values)\n\nTemporal\n\nRevisit time of sensor\nHigh resolution / pixel -> low revisit = low cost and number [graph]\n\n\nTrade off between resolution and time (and cost)\n\n\n1.1.3 Practical\nOverview/Workflow (SNAP):\nDownload -> New project in SNAP -> Save downloaded file in a Data folder in the same directory as the project -> Open product/zipped data -> Select RGB-image channel (on product explorer panel right click product) => Fig. a,b,c\nTo inspect/change image display: -> Colour Manipulation panel (View -> Tool Windows) -> Change the range of histogram\nTo analyse spectral feature space: -> Scatter plot (under Analysis tab) => Fig. d\nTo mask the study area: -> Resampling to 20m (since both masking and the Tasseled Cap function require same resolution while B2/B3/B4/B8 are 10m and B11/B12 are 20m) -> Import vector (ESRI shapefile) -> Select the imported layer in layer manager -> Masking under Raster - Masks - Land/Sea Mask (select bands that would be used)\nTo reduce dimensionality via Tasseled Cap transformation : -> Apply transformation functions in Band Math -> Select RGB-image channel => Fig. e,f\nSince Landsat has different spatial resolution than Sentinel, the latter will be resampled again (upscale). And since the Sentinel data has been masked/selected useful bands for Tasseled cap, it will be masked again to select B1-B7.\nTo compare spectral signature: -> Make sure both data in the same resolution with the same bands -> Add polygons by land cover -> Export polygons as shapefiles -> Export both dataset as GeoTIFF -> Check in QGIS -> Compare in R using terra (or stars)\n\n1.1.3.1 Sentinel and Landsat\n\nData availability depends on cloud cover, time, weather (sometimes study area may not have data under the filtered criteria)\nSentinel tends to have shorter download time than Landsat.\nLandsat has less available data in a given time range (due to slightly longer revisit time).\n\n\nSentinel and Landsat data in QGIS and SNAP\n\n\n\nSentinel\nLandsat\n\n\n\n\nSpatial resolution\nResolutions varied for each bands. (E.g. 10m resolution for Band 2)\n30m resolution\n\n\nTemporal resolution\n10 days (5 days for combined constellation 2A & 2B)\n16 days\n\n\nCentral wavelength\nB1 443nm = Band 1 has central wavelength of 442 nm\n\n\n\nProcess by Software\nQGIS\nSNAP\n\n\nColour composite\nmerge the BOA bands (B2, B3, B4, B8) to make true colour composite (B1=Blue, B2=Green, B3=Red)\nrecreate by changing RBG channels (Fig. a, b, c)\n\n\nEnhancement\nContrast enhancement\nImage histogram\n\n\nSpectral feature space\n\nScatter plot (Fig. d)\n\n\nResampling\n\nB11 and B12 are at a 20m resolution whereas all the others at a 10m resolution. -> resample others to 20m (upscale) Sentinel 2 resampling toolbox\n\n\n\n\nA) Traditional resample: considers the neighbouring pixels (time-efficient?)\nB) Sentinel 2 products resample: account for the particularities of the angle (satellite viewing) bands\n\n\nMasking\n\nMasks. Land/sea mask.\nCan only mask bands on the same resolution\n\n\nTasseled Cap function\n\nBand Maths.\nReduce dimensionality: (similar to PCA) spectral index combining two or more bands to highlight certain features of an image. Then change RGB channel to those new data to show results (Fig. e)\n\n\n\n\n\n1.1.3.2 Masking vs. cropping\n\nmasking: the outline of the geo boundary\ncropping: to the extent, the rectangular parameter of the geo boundary\n\n\n\n1.1.3.3 Example: Processing of Sentinel and Landsat data of Kinmen County\nI’ve chosen Kinmen County in Taiwan as my case study area for practical. It consists of several islands and is in close proximity to Xiamen, China. The main/largest island is the H-shaped one at the lower end of the first few maps (or see Fig. e).\nThis is where i spent the majority of summer in 2022 so i’ve got some knowledge of the city/island’s land use. I also cycled a lot on this island (so having some sense of the topography). It is really enjoyable to cycle here as there is less traffic and more greenery. However, it tends to be slightly cloudy/rainy in certain months which may constrain the analysis. Anyhow, let’s see how it goes.\n\n1.1.3.3.1 Sentinel\nThe initial exploration using Sentinel data:\n\n\n\n\n\n\na) Natural colour\n\n\n\n\n\n\n\nb) False colour composite\n\n\n\n\n\n\n\n\n\nc) Atmospheric penetration composite\n\n\n\n\n\n\n\nd) Scatter plot (B4 Red - B8 NIR)\n\n\n\n\n\n\nFalse colour composite: B8, B4, B3. Plants reflect near-infrared and green light whilst absorbing red.\nAtmospheric penetration composite: B12, B11, B8A with no visible bands to penetrate atmospheric particles. Vegetation = blue, urban area = white, gray cyan or purple.\n\nBasically, Fig a,b,c show that the coastal city Xiamen’s built-up areas are mostly close to the seashore. This is mainly due to its historical ports that are open to international trades and thus becoming more urbanized. Kinmen Islands are less populous compared to Xiamen and the majority of land is bare soil / vegetation. Fig d shows that the dry bare soil is prominent in some areas (black dots at high B4 and high B8), while the image has large ratio of biomass (yellow peak at low B4 and high B8).\n\n\n1.1.3.3.2 Tasseled Cap function\nArcPro’s explanation:\nThe function is originally designed to monitor crop’s life cycle / changes through time.\n\nBrightness = bare or partially covered soil, man-made, and natural features such as concrete, asphalt, gravel, rock outcrops, and other bare areas\nGreenness = green vegetation\nWetness = soil moisture, water, and other moist features.\n\n✔️ Those three components of imagery contain about 97% of the meaningful information available in the image (removing noise and atmospheric effects).\n\n\n\n\n\n\ne) PCA / Tasseled Cap function transformed (May 2022)\n\n\n\n\n\n\n\nf) PCA / Tasseled Cap function transformed (November 2022)\n\n\n\n\n\n(Darker) blue and red areas are generally unchanged - representing water (lakes/reservoir/sea), soil and built-up area. The pink areas are built-up areas / concrete / asphalt / human-made (e.g. southern inner bay’s long linear shape is the airport, and western area on the main island is the most populous town)\nGreenness in Fig. e is less than that in Fig. f, indicating the vegetation is less in May. During May (or generally spring time), there should be roughly similar amount of vegetation / crops, compared to November (autumn), while the crops during autumn reach maturity leading to more greenness in the latter figure.\nIt might also be the case that the light blue areas are soil moisture / higher humidity. As can be seen from both figures, the shaded blue areas in the central/slightly eastern-central location of the largest island is mountainous. The humidity can be higher there.\nAnother difference between the two images is that the redness is more intense in the first figure (more NIR?), while more organgy in the second. This indicates that the those areas are soil or bare areas. During autumn, the vegetation on those land increase, so the yellowness/greenness overlays.\n\n\n1.1.3.3.3 Landsat\nTo mitigate the atmospheric effects (cloudy and foggy from April to May and rainy during summer) (and also due to data availability), the sensing time for Landsat is in December.\n\n\n1.1.3.3.4 Compare spectral signatures\n\n\n\n\n\n\nSpectral reflectance Sentinel\n\n\n\n\n\n\n\nSpectral reflectance Landsat\n\n\n\n\n\nThe ranges of the datasets are different visually. Landsat has higher mean reflectance for all land cover in all bands.\nFor both sensors, the relationship between each land cover is similar, with water, forest and grass having lowest reflectance, and albeit less visible in Sentinel, bare earth having highest reflectance.\nThen, t-test is conducted to test the difference in average of the land cover reflectance between sensors.\n\nWelch two sample t-test (Sentinel and Landsat)\n\n\nlandcover\nt\ndf\np-value\n\n\n\n\nlow_urban\n-136.65\n2024.3\n< 2.2e-16\n\n\nhigh_urban\n-299.51\n6706.5\n< 2.2e-16\n\n\nbare_earth\n-238.64\n6347.2\n< 2.2e-16\n\n\ngrass\n-76.054\n601.63\n< 2.2e-16\n\n\nforest\n-269.72\n7087.8\n< 2.2e-16\n\n\nwater\n-157.95\n348.97\n< 2.2e-16\n\n\n\nSince p-values are all below 0.05, the difference in average reflectance of Landsat and of Sentinel data is statistically significant.\n\n\n\n1.1.3.4 QGIS vs. SNAP\nQGIS:\n\neasy to check stuff / visualize outputs\n\nSNAP: pre-processing and analysing remotely sensed / raster data. For sentinel and other sensors comparison\n\nEasier to recreate TCC\nCan manually set the histogram / contract enhancement as in QGIS\nHaving data from multiple sensors in the same software that can be explored together\nbut it’s a pain to reopen a project.. and I found when setting up the project, it’s more straightforward to create a new project before opening up a product.\n\nTwo software can work together (using QGIS to check everything as did in CASA0005).\n\nwhen i checked GeoTIFFs and shapefiles in QGIS, nothing is shown.. so probably something went wrong when selecting POIs.\n\n\n\n\n1.1.4 Questions\n\nwhy in the practical book before Landsat when resampling Sentinel, the equation for brightness does not include B11 and B12 (but using 0.5082 and 0.1863)?\nA: It should be B11 and B12.\nFig e looks different in terms of colour scheme? Since the majority of the city is coloured in blue, what do the blueish areas represent/mean (physically those areas are vegetations/forests, shouldn’t those be in green)?\nA: It depends on the colour gun set by the RGB."
  },
  {
    "objectID": "1_intro.html#application",
    "href": "1_intro.html#application",
    "title": "1  Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application"
  },
  {
    "objectID": "1_intro.html#reflection",
    "href": "1_intro.html#reflection",
    "title": "1  Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023_LD",
    "section": "",
    "text": "Preface\nThis diary is for CASA0023 Remote Sensing Cities and Environments 22/23.\nemail: zcfther@ucl.ac.uk"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "CASA0023_LD",
    "section": "Content",
    "text": "Content\n\nIntroduction\n\n\nRemote sensing intro / sensors / resolutions\n\n\nSensor - WorldView3\nCorrections"
  },
  {
    "objectID": "index.html#structure-of-each-chapter",
    "href": "index.html#structure-of-each-chapter",
    "title": "CASA0023_LD",
    "section": "Structure (of each chapter)",
    "text": "Structure (of each chapter)\n\nSummary:\nContent summary (outputs from the practical, small code chunks with relevant explanation and flow charts)\nQuestions (data, methods, or applications)\nApplications:\nApplication of data / concepts / methods in literature / policy (largely sourced from weekly reading).\nReflection:\nA personal reflection on the presented content (e.g. what was interesting, why and why might the data or tools presented this week be useful in the future to you or perhaps they won’t be useful but something similar might be)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "CASA0023_LD",
    "section": "Education",
    "text": "Education\nUniversity College London"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "CASA0023_LD",
    "section": "Professional experience",
    "text": "Professional experience"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "CASA0023_LD",
    "section": "Interests",
    "text": "Interests"
  },
  {
    "objectID": "index.html#what-i-hope-to-get-from-the-module",
    "href": "index.html#what-i-hope-to-get-from-the-module",
    "title": "CASA0023_LD",
    "section": "What I hope to get from the module",
    "text": "What I hope to get from the module"
  },
  {
    "objectID": "2_portfolio.html#summary-application-and-reflection",
    "href": "2_portfolio.html#summary-application-and-reflection",
    "title": "2  Sensor-WorldView3",
    "section": "2.1 Summary, Application and Reflection",
    "text": "2.1 Summary, Application and Reflection\nA basic introduction on WorldView-3."
  },
  {
    "objectID": "intro.html#structure-of-each-chapter",
    "href": "intro.html#structure-of-each-chapter",
    "title": "Content",
    "section": "Structure (of each chapter)",
    "text": "Structure (of each chapter)\n\nSummary:\n\nContent summary (outputs from the practical, small code chunks with relevant explanation and flow charts)\n\n✔️&❌: indicate Pros and Cons of specific idea / concept / object…\n\nQuestions (data, methods, or applications)\n\nApplications:\n\nApplication of data / concepts / methods in literature / policy (largely sourced from weekly reading).\n\nReflection:\n\nA personal reflection on the presented content (e.g. what was interesting, why and why might the data or tools presented this week be useful in the future to you or perhaps they won’t be useful but something similar might be)"
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Summary"
  },
  {
    "objectID": "1_intro.html#sensors",
    "href": "1_intro.html#sensors",
    "title": "1  Introduction",
    "section": "2.1 Sensors",
    "text": "2.1 Sensors\n\nPassive (Sun energy reflection in EM wave, don’t emit EM waves)\n\nExample: human eyes, satellite\nCon: Atmosphere Haze (require atmosphere correction, no haze in outer space due to no atmosphere) and scattering (wavelength (e.g. blue sky and orange sunset)), clouds and weather\n\nActive (emit and receive EM wave / energy)\n\nExample: radar, x-ray, LiDAR\nPro: can pass clouds"
  },
  {
    "objectID": "1_intro.html#four-resolutions",
    "href": "1_intro.html#four-resolutions",
    "title": "1  Introduction",
    "section": "2.2 Four resolutions",
    "text": "2.2 Four resolutions\n\nSpatial\n\nPro: fast\nCon: costly\n\nSpectral\n\nSpectral signature (EM): Objects on Earth have difference wavelengths = Bands\n\nMulti-spectral data / image\nHyperspectral data / image: stack all colour bands\n\nTrue colour (human eye can see) and false colour (human cant see)\nCon: atmospheric window (atmospheric transmission / opacity, e.g. water vapour, ozone, CO…)\nSpectroradiometer\n\nRadiometric\n\nCell/ pixel\nIncrease bit -> increase possible values\n8-bit\n\nTemporal\n\nRevisit time of sensor\nHigh resolution / pixel -> low revisit = low cost and number [graph]\n\n\nTrade off between resolution and time (and cost)\n\n2.2.1 Questions"
  },
  {
    "objectID": "3_corrections.html#summary",
    "href": "3_corrections.html#summary",
    "title": "3  Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Corrections\n\nGeometric Corrections\n\nTo reduce geometric distortion\nOff-Nadir -> Nadir\n\n\nGround central point: GCP\n\nA ground control point (GCP) is a location on the surface of the Earth (e.g., a road intersection) that can be identified on the imagery and located accurately on a map.\n(Objects (e.g. building) on the image that do not move, as the reference point of corrections.)\n\n\n\nCoordinates of each GCP -> linear regression\nReduce error <- increase GCP\nRequirement for corrections:\n\nsame resolution (resampling)\nsame CRS (reprojecting)\n\n\n\nImage-to-map rectification (rectify remotely sensed data to a standard map projection)\nImage-to-image registration (remotely sensed data used in conjunction with other spatial information in a GIS)\n\n\nForward (input-to-output) mapping\n\nForward mapping\n\nX: original image -> Y: target image\n\n\n\n✔️rectify the location of discrete coordinates found along a linear feature such as a road in a vector map\n❌possibility of points outside ‘gold standard’ (the rectified/targeted image) -> output matrix pixel with no output value\n\nInverse (output-to-input) mapping\n\nBackward mapping\n\nX: target image -> Y: original image\n\n\n✔️ use points on ‘gold standard’ to match point on original data\n\n\n\n\nForward and inverse mapping (Jensen, 2015)\n\n\n\nMoisaicking\n\nMosaicking\n\nMoisaicking is the process of combining multiple images into a single seamless composite image.\n\n\n\ncut-line feathering: offset the edge of images by certain distance\nedge feathering: specify objects (e.g. roads/rivers) as edge\n\n\nAtmospheric Corrections\n\nTo mitigate the effect of scattering and absorption & to avoid loss of reflectance & signiture extension thorough space and time (Jensen 2015)\nPoint Spread Function\n\nPoint Spread Function\n\nMeasured and modeled point spread functions (PSF) of sensor systems indicate that a significant portion of the recorded signal of each pixel of a satellite image originates from outside the area represented by that pixel. This hinders the ability to derive surface information from satellite images on a per-pixel basis (Huang et al. 2002).\n\n\nRelative\n1) to normalize the intensities among the different bands within a single-date remotely sensed image, and 2) to normalize the intensities of bands of remote sensor data in multiple dates of imagery to a standard scene selected by the analyst.\n\nDark object subtraction (DOS)\n❌ assuming unusual brightness of darkest pixel as atmosphere\nMultiple-date image normalization using regression\n\nPseudo-Invariant Features (PIFs) selection = radiometric GCP\nRequirements of PIF: 1) little changes through time; 2) similar elevation as other land in scene; 3) minimal vegetation; 4) in a relatively flat area.\n\nUse middle of years, Y: base image\n\nAbsolute\n❌ High data requirement (fieldwork/…); High software requirement (costy)\n✔️Digital counts in satellite / aircraft image data -> scaled surface reflectance\n\nEmpirical Line Calibration (ELC)\n\nRequirements:\n\nTwo or more areas in the scene with different albedos (e.g., one bright target such as sand and one dark target such as a deep, nonturbid water body) & as homogeneous as possible <- difficult.\nSensor calibration coefficients\nRadiative transfer code (knowledge of sensor spectral profile & atmospheric properties at the time of data collection) (Jensen 2015)\n\n\n\n\nTopographic Corrections (Orthorectification)\n\nTo remove topographically induced illumination variation\n\nIllumination\n\nthe cosine of the incident solar angle, thus representing the proportion of the direct solar radiation hitting a pixel.\n\n\nRequirements: sensor geometry & elevation model\nShould be done after atmospheric corrections\nCosine Function: \\(LH=LT\\frac{cosθO}{cosi}\\)\n\nZenith\n\nThe solar zenith angle is the zenith angle of the sun, i.e., the angle between the sun’s rays and the vertical direction. Solar zenith angle is normally used in combination with the solar azimuth angle to determine the position of the Sun as observed from a given location on the surface of the Earth.\n\nAzimuth\n\nThe solar azimuth angle is the azimuth (horizontal angle with respect to north) of the Sun’s position.\n\n\n\n\n\n\nZenith angle and cosine function (Jensen, 2015)\n\n\n\n❌ not considering diffuse skylight / light reflected from surrounding mountainsides\nsmaller cos i -> greater over-correction (Jensen 2015)\n\nRadiometric Calibrations\n\nDigital number of each pixel = raw, unitless\nRegression = unit\nReflectance (BOA) = comparable\nTOA radiance -> TOA reflectance = no light\nSurface reflectance = no light, no atmosphere\nHemispherical reflectance (e.g. in Labs)\nApparent reflectance\n\n\n\n\n3.1.2 Enhancement\n\nFeathering / joining\n\nGEE: median\nsurface reflectance data not comparable -> standardization, normalization\n\nImage Enhancement\n\nContract enhancement (QGIS)\nnot changing data, but changing display\nRatioing\ndivide / compare bands with each other, normalized surface reflectance\nFiltering\nPCA\nTexture analysis\n1st order occurrence: ✔️classification\nEdge of building enhanced via 1st order variance\n2nd order co-occurrence: ✔️classification improvement and additional info (to other bands)\ncan then be used in PCA\nFusion\nCombine bands from different sensors / texture analysis layer\nResampling - different pixel/location\n\nDecision\nObject\nImage\nPan sharpen: for better visualization\n\n\n\n\n\n3.1.3 Questions"
  },
  {
    "objectID": "3_corrections.html#application",
    "href": "3_corrections.html#application",
    "title": "3  Corrections",
    "section": "3.2 Application",
    "text": "3.2 Application\n\n3.2.1 Corrections\n\nGeometric Corrections: Redlining in the US.\nThe historical maps/plans were drawn by hands. Those maps were digitized through geometric corrections, allowing the research/manipulation of this database on housing policy relating to socio-economic wellbeing and zoning.\n\n\n\nRedlining historic map corrections"
  },
  {
    "objectID": "3_corrections.html#reflection",
    "href": "3_corrections.html#reflection",
    "title": "3  Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nDifferent methods of corrections are interlinked, finding some reference points/signatures to rectify the image. The detailed processes (like regressions and data collection methods) are difficult to digest (probably because not being practiced) while the software is available for corrections.\n\n\n\n\nHuang, Chengquan, John R. G. Townshend, Shunlin Liang, Satya N. V. Kalluri, and Ruth S. DeFries. 2002. “Impact of Sensor’s Point Spread Function on Land Cover Characterization: Assessment and Deconvolution.” Remote Sensing of Environment 80 (2): 203–12. https://doi.org/10.1016/S0034-4257(01)00298-X.\n\n\nJensen, John R. 2015. Introductory Digital Image Processing. 4th ed. A Remote Sensing Perspective. Pearson Higher Education US."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dictionary.html",
    "href": "dictionary.html",
    "title": "5  Dictionary",
    "section": "",
    "text": "6 Dictionary / Terminology\nGround central point: GCP"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Jensen, John R. 2015. Introductory Digital Image Processing.\n4th ed. A Remote Sensing Perspective. Pearson Higher Education US."
  }
]