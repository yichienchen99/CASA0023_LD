[
  {
    "objectID": "1_intro.html#summary",
    "href": "1_intro.html#summary",
    "title": "1  Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 Sensors\n\nPassive (Sun energy reflection in EM wave, don’t emit EM waves)\n\nExample: human eyes, satellite (e.g. Wk2 sensor summary - WorldView-3)\n❌ Influenced by atmosphere Haze (require atmosphere correction, no haze in outer space due to no atmosphere) and scattering (wavelength (e.g. blue sky and orange sunset)), clouds and weather\n✔️does not disturb the object or area of interest\n\nActive (emit and receive EM wave / energy)\n\nExample: radar, x-ray, LiDAR\n✔️ can pass clouds\n\n\n\n\n1.1.2 Four resolutions\n\nSpatial\n\nthe size of the raster grid per pixel (e.g. 20cm or 30m)\nLow spatial resolution: fast (short revisit time); High spatial resolution: costly\n\nSpectral\n\nSpectral signature (EM): Objects on Earth have difference wavelengths = Bands\n\nMulti-spectral data / image\nHyperspectral data / image: stack all colour bands\n\nTrue colour (human eye can see) and false colour (human cant see)\n❌ atmospheric window (atmospheric transmission / opacity, e.g. water vapour, ozone, CO…)\nSpectroradiometer\n\nRadiometric\n\nidentify differences in light or reflectance, in practice this is the range of possible values.\nIncrease bit -> increase possible values\n8-bit sensor: 0-255; 11-bit sensor: 0-2047\n\nTemporal\n\nRevisit time of sensor\nHigh resolution / pixel -> low revisit = low cost and number [graph]\n\n\nTrade off between resolution and time (and cost)\n\n\n1.1.3 Practical: Sentinel and Landsat\n\nSentinel and Landsat data in QGIS and SNAP\n\n\n\n\n\n\n\n\n\nSentinel\n\nLandsat\n\n\n\n\nDownload\nData availability depends on cloud cover, time, weather (sometimes study area may not have data under the filtered criteria)\n\n\n\n\n\nSentinel-2 is 12-bit (brightness levels from 0 – 4095) (beyond True Colour Image (TCI) values)\n\n30m resolution\n\n\nSoftware\nQGIS\nSNAP\n\n\n\nColour composite\nmerge the BOA bands (B2, B3, B4, B8) to make true colour composite (B1=Blue, B2=Green, B3=Red)\nrecreate by changing RBG channels (Fig. a, b, c)\n\n\n\nEnhancement\nContract enhancement\nImage histogram\n\n\n\nSpectral feature space\n\nScatter plot (Fig. d)\n\n\n\nResampling\n\nB11 and B12 are at a 20m resolution whereas all the others at a 10m resolution. -> resample others to 20m (upscale) Sentinel 2 resampling toolbox\n\n\n\n\n\nA) Traditional resample: considers the neighbouring pixels (time-efficient?)\nB) Sentinel 2 products resample: account for the particularities of the angle (satellite viewing) bands\n\n\n\nMasking\n\nCan only mask bands on the same resolution\n\n\n\nTasseled Cap function\n\nBand Maths,\nReduce dimensionality: (similar to PCA) spectral index combining two or more bands to highlight certain features of an image. Then change RGB channel to those new data to show results (Fig. e)\n\n\n\nNew vector POI\n\n\n\n\n\nExport\n\n\n\n\n\nComparison\n\n\n\n\n\n\n\n\n1.1.4 Masking vs cropping:\n\nmasking: the outline of the geo boundary\ncropping: to the extent, the rectangular parameter of the geo boundary\n\n\n\n\n\n\n\na) Natural colour\n\n\n\n\n\n\n\nb) false colour composite\n\n\n\n\n\n\n\n\n\nc) Atmospheric penetration composite\n\n\n\n\n\n\n\nd) Scatter plot (B4 Red - B8 NIR)\n\n\n\n\n\n\n\n\ne) PCA / Tasseled Cap function transformed\n\n\n\n\n1.1.5 QGIS vs. SNAP\no QGIS:\n\neasy to check stuff\n\no SNAP: pre-processing and analysing remotely sensed / raster data. For sentinel and other sensors comparison\n\nEasier to recreate TCC\nCan manually set the histogram / contract enhancement as in QGIS\nhaving data from multiple sensors in the same software that can be explored together\n\n\n\n1.1.6 Questions\n\nwhy in the practical book before Landsat when resampling Sentinel, the equation for brightness does not include B11 and B12 (but using 0.5082 and 0.1863)?\nA: It should be B11 and B12.\nFig e looks different in terms of colour scheme? Since the majority of the city is coloured in blue, what do the blueish areas represent/mean (physically those areas are vegetations/forests, shouldn’t those be in green)?\nA: It depends on the colour gun set by the RGB."
  },
  {
    "objectID": "1_intro.html#application",
    "href": "1_intro.html#application",
    "title": "1  Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application"
  },
  {
    "objectID": "1_intro.html#reflection",
    "href": "1_intro.html#reflection",
    "title": "1  Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023_LD",
    "section": "",
    "text": "Preface\nThis diary is for CASA0023 Remote Sensing Cities and Environments 22/23.\nemail: zcfther@ucl.ac.uk"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "CASA0023_LD",
    "section": "Content",
    "text": "Content\n\nIntroduction\n\n\nRemote sensing intro / sensors / resolutions\n\n\nSensor - WorldView3\nCorrections"
  },
  {
    "objectID": "index.html#structure-of-each-chapter",
    "href": "index.html#structure-of-each-chapter",
    "title": "CASA0023_LD",
    "section": "Structure (of each chapter)",
    "text": "Structure (of each chapter)\n\nSummary:\nContent summary (outputs from the practical, small code chunks with relevant explanation and flow charts)\nQuestions (data, methods, or applications)\nApplications:\nApplication of data / concepts / methods in literature / policy (largely sourced from weekly reading).\nReflection:\nA personal reflection on the presented content (e.g. what was interesting, why and why might the data or tools presented this week be useful in the future to you or perhaps they won’t be useful but something similar might be)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "CASA0023_LD",
    "section": "Education",
    "text": "Education\nUniversity College London"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "CASA0023_LD",
    "section": "Professional experience",
    "text": "Professional experience"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "CASA0023_LD",
    "section": "Interests",
    "text": "Interests"
  },
  {
    "objectID": "index.html#what-i-hope-to-get-from-the-module",
    "href": "index.html#what-i-hope-to-get-from-the-module",
    "title": "CASA0023_LD",
    "section": "What I hope to get from the module",
    "text": "What I hope to get from the module"
  },
  {
    "objectID": "2_portfolio.html#summary-application-and-reflection",
    "href": "2_portfolio.html#summary-application-and-reflection",
    "title": "2  Sensor-WorldView3",
    "section": "2.1 Summary, Application and Reflection",
    "text": "2.1 Summary, Application and Reflection\nA basic introduction on WorldView-3."
  },
  {
    "objectID": "intro.html#structure-of-each-chapter",
    "href": "intro.html#structure-of-each-chapter",
    "title": "Content",
    "section": "Structure (of each chapter)",
    "text": "Structure (of each chapter)\n\nSummary:\n\nContent summary (outputs from the practical, small code chunks with relevant explanation and flow charts)\n\n✔️&❌: indicate Pros and Cons of specific idea / concept / object…\n\nQuestions (data, methods, or applications)\n\nApplications:\n\nApplication of data / concepts / methods in literature / policy (largely sourced from weekly reading).\n\nReflection:\n\nA personal reflection on the presented content (e.g. what was interesting, why and why might the data or tools presented this week be useful in the future to you or perhaps they won’t be useful but something similar might be)"
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Summary"
  },
  {
    "objectID": "1_intro.html#sensors",
    "href": "1_intro.html#sensors",
    "title": "1  Introduction",
    "section": "2.1 Sensors",
    "text": "2.1 Sensors\n\nPassive (Sun energy reflection in EM wave, don’t emit EM waves)\n\nExample: human eyes, satellite\nCon: Atmosphere Haze (require atmosphere correction, no haze in outer space due to no atmosphere) and scattering (wavelength (e.g. blue sky and orange sunset)), clouds and weather\n\nActive (emit and receive EM wave / energy)\n\nExample: radar, x-ray, LiDAR\nPro: can pass clouds"
  },
  {
    "objectID": "1_intro.html#four-resolutions",
    "href": "1_intro.html#four-resolutions",
    "title": "1  Introduction",
    "section": "2.2 Four resolutions",
    "text": "2.2 Four resolutions\n\nSpatial\n\nPro: fast\nCon: costly\n\nSpectral\n\nSpectral signature (EM): Objects on Earth have difference wavelengths = Bands\n\nMulti-spectral data / image\nHyperspectral data / image: stack all colour bands\n\nTrue colour (human eye can see) and false colour (human cant see)\nCon: atmospheric window (atmospheric transmission / opacity, e.g. water vapour, ozone, CO…)\nSpectroradiometer\n\nRadiometric\n\nCell/ pixel\nIncrease bit -> increase possible values\n8-bit\n\nTemporal\n\nRevisit time of sensor\nHigh resolution / pixel -> low revisit = low cost and number [graph]\n\n\nTrade off between resolution and time (and cost)\n\n2.2.1 Questions"
  },
  {
    "objectID": "3_corrections.html#summary",
    "href": "3_corrections.html#summary",
    "title": "3  Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Corrections\n\nGeometric Corrections\n\nTo reduce geometric distortion\nOff-Nadir -> Nadir\n\n\nGround central point: GCP\n\nA ground control point (GCP) is a location on the surface of the Earth (e.g., a road intersection) that can be identified on the imagery and located accurately on a map.\n(Objects (e.g. building) on the image that do not move, as the reference point of corrections.)\n\n\n\nCoordinates of each GCP -> linear regression\nReduce error <- increase GCP\nRequirement for corrections:\n\nsame resolution (resampling)\nsame CRS (reprojecting)\n\n\n\nImage-to-map rectification (rectify remotely sensed data to a standard map projection)\nImage-to-image registration (remotely sensed data used in conjunction with other spatial information in a GIS)\n\n\nForward (input-to-output) mapping\n\nForward mapping\n\nX: original image -> Y: target image\n\n\n\n✔️rectify the location of discrete coordinates found along a linear feature such as a road in a vector map\n❌possibility of points outside ‘gold standard’ (the rectified/targeted image) -> output matrix pixel with no output value\n\nInverse (output-to-input) mapping\n\nBackward mapping\n\nX: target image -> Y: original image\n\n\n✔️ use points on ‘gold standard’ to match point on original data\n\n\n\n\nForward and inverse mapping (Jensen, 2015)\n\n\n\nMoisaicking\n\nMosaicking\n\nMoisaicking is the process of combining multiple images into a single seamless composite image.\n\n\n\ncut-line feathering: offset the edge of images by certain distance\nedge feathering: specify objects (e.g. roads/rivers) as edge\n\n\nAtmospheric Corrections\n\nTo mitigate the effect of scattering and absorption & to avoid loss of reflectance & signiture extension thorough space and time (Jensen 2015)\nPoint Spread Function\n\nPoint Spread Function\n\nMeasured and modeled point spread functions (PSF) of sensor systems indicate that a significant portion of the recorded signal of each pixel of a satellite image originates from outside the area represented by that pixel. This hinders the ability to derive surface information from satellite images on a per-pixel basis (Huang et al. 2002).\n\n\nRelative\n1) to normalize the intensities among the different bands within a single-date remotely sensed image, and 2) to normalize the intensities of bands of remote sensor data in multiple dates of imagery to a standard scene selected by the analyst.\n\nDark object subtraction (DOS)\n❌ assuming unusual brightness of darkest pixel as atmosphere\nMultiple-date image normalization using regression\n\nPseudo-Invariant Features (PIFs) selection = radiometric GCP\nRequirements of PIF: 1) little changes through time; 2) similar elevation as other land in scene; 3) minimal vegetation; 4) in a relatively flat area.\n\nUse middle of years, Y: base image\n\nAbsolute\n❌ High data requirement (fieldwork/…); High software requirement (costy)\n✔️Digital counts in satellite / aircraft image data -> scaled surface reflectance\n\nEmpirical Line Calibration (ELC)\n\nRequirements:\n\nTwo or more areas in the scene with different albedos (e.g., one bright target such as sand and one dark target such as a deep, nonturbid water body) & as homogeneous as possible <- difficult.\nSensor calibration coefficients\nRadiative transfer code (knowledge of sensor spectral profile & atmospheric properties at the time of data collection) (Jensen 2015)\n\n\n\n\nTopographic Corrections (Orthorectification)\n\nTo remove topographically induced illumination variation\n\nIllumination\n\nthe cosine of the incident solar angle, thus representing the proportion of the direct solar radiation hitting a pixel.\n\n\nRequirements: sensor geometry & elevation model\nShould be done after atmospheric corrections\nCosine Function: \\(LH=LT\\frac{cosθO}{cosi}\\)\n\nZenith\n\nThe solar zenith angle is the zenith angle of the sun, i.e., the angle between the sun’s rays and the vertical direction. Solar zenith angle is normally used in combination with the solar azimuth angle to determine the position of the Sun as observed from a given location on the surface of the Earth.\n\nAzimuth\n\nThe solar azimuth angle is the azimuth (horizontal angle with respect to north) of the Sun’s position.\n\n\n\n\n\n\nZenith angle and cosine function (Jensen, 2015)\n\n\n\n❌ not considering diffuse skylight / light reflected from surrounding mountainsides\nsmaller cos i -> greater over-correction (Jensen 2015)\n\nRadiometric Calibrations\n\nDigital number of each pixel = raw, unitless\nRegression = unit\nReflectance (BOA) = comparable\nTOA radiance -> TOA reflectance = no light\nSurface reflectance = no light, no atmosphere\nHemispherical reflectance (e.g. in Labs)\nApparent reflectance\n\n\n\n\n3.1.2 Enhancement\n\nFeathering / joining\n\nGEE: median\nsurface reflectance data not comparable -> standardization, normalization\n\nImage Enhancement\n\nContract enhancement (QGIS)\nnot changing data, but changing display\nRatioing\ndivide / compare bands with each other, normalized surface reflectance\nFiltering\nPCA\nTexture analysis\n1st order occurrence: ✔️classification\nEdge of building enhanced via 1st order variance\n2nd order co-occurrence: ✔️classification improvement and additional info (to other bands)\ncan then be used in PCA\nFusion\nCombine bands from different sensors / texture analysis layer\nResampling - different pixel/location\n\nDecision\nObject\nImage\nPan sharpen: for better visualization\n\n\n\n\n\n3.1.3 Questions"
  },
  {
    "objectID": "3_corrections.html#application",
    "href": "3_corrections.html#application",
    "title": "3  Corrections",
    "section": "3.2 Application",
    "text": "3.2 Application\n\n3.2.1 Corrections\n\nGeometric Corrections: Redlining in the US.\nThe historical maps/plans were drawn by hands. Those maps were digitized through geometric corrections, allowing the research/manipulation of this database on housing policy relating to socio-economic wellbeing and zoning.\n\n\n\nRedlining historic map corrections"
  },
  {
    "objectID": "3_corrections.html#reflection",
    "href": "3_corrections.html#reflection",
    "title": "3  Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nDifferent methods of corrections are interlinked, finding some reference points/signatures to rectify the image. The detailed processes (like regressions and data collection methods) are difficult to digest (probably because not being practiced) while the software is available for corrections.\n\n\n\n\nHuang, Chengquan, John R. G. Townshend, Shunlin Liang, Satya N. V. Kalluri, and Ruth S. DeFries. 2002. “Impact of Sensor’s Point Spread Function on Land Cover Characterization: Assessment and Deconvolution.” Remote Sensing of Environment 80 (2): 203–12. https://doi.org/10.1016/S0034-4257(01)00298-X.\n\n\nJensen, John R. 2015. Introductory Digital Image Processing. 4th ed. A Remote Sensing Perspective. Pearson Higher Education US."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dictionary.html",
    "href": "dictionary.html",
    "title": "5  Dictionary",
    "section": "",
    "text": "6 Dictionary / Terminology\nGround central point: GCP"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Jensen, John R. 2015. Introductory Digital Image Processing.\n4th ed. A Remote Sensing Perspective. Pearson Higher Education US."
  }
]